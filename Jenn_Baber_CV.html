<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>
        
    </title>
    <link rel="stylesheet"
        href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/5.5.1/github-markdown-light.min.css"
        integrity="sha512-Pmhg2i/F7+5+7SsdoUqKeH7UAZoVMYb1sxGOoJ0jWXAEHP0XV2H4CITyK267eHWp2jpj7rtqWNkmEOw1tNyYpg=="
        crossorigin="anonymous" referrerpolicy="no-referrer" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.12/dist/katex.min.css" integrity="sha384-PDbUeNCuE6bOPudPOgFyIUEy3UJawJVwr3XlGO90FIuf5qNIoTLSgOJo/dC2ZXV/" crossorigin="anonymous">

    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.12/dist/katex.min.js" integrity="sha384-VkqWq8xtm5YQk1BBXczQ8/Sx+DlCzF8cuS43bZwmtVXzRFtyLTqTCdP7MKmKo+KN" crossorigin="anonymous"></script>

    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.12/dist/contrib/auto-render.min.js" integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous"
        onload="renderMathInElement(document.body, {delimiters: [{ left: '$$',  right: '$$',  display: false }]});"></script>
    <style>
        .markdown-body {
            box-sizing: border-box;
            min-width: 200px;
            max-width: 980px;
            margin: 0 auto;
            padding: 45px;
        }

        @media (max-width: 767px) {
            .markdown-body {
                padding: 15px;
            }
        }
    </style>
</head>

<body>
    <article class="markdown-body">
        <h1>Jenn Baber's CV</h1>
        <ul>
        <li>Phone: +1 972 742 1453</li>
        <li>Email: <a href="mailto:jennifer.baber@outlook.com">jennifer.baber@outlook.com</a></li>
        <li>Location: Richardson, TX</li>
        <li>LinkedIn: <a href="https://linkedin.com/in/jennifer-baber-6969781b0">jennifer-baber-6969781b0</a></li>
        <li>GitHub: <a href="https://github.com/jenn-baber-09">jenn-baber-09</a></li>
        </ul>
        <h1>Overview</h1>
        <p><strong>Lead Data Engineer with a strong mathematics foundation and a product-driven mindset,</strong> specializing in scalable data pipelines, analytics engineering, and ML/AI-ready data platforms. Experienced in Snowflake, dbt, Python, SQL, ETL/ELT, data modeling, and DataOps, with <strong>a track record of building production-grade systems that are reliable, cost-efficient, and trusted by stakeholders</strong>. Strong cross-functional partner known for modernizing and maturing data architectures, enabling machine learning use cases, and delivering business-impactful insights at scale.</p>
        <h1>Technical Proficiencies</h1>
        <p><strong>Data Engineering:</strong> dbt, Snowflake, ETL, ELT, data pipelines, data modeling, dimensional modeling, data contracts, DataOps, CI/CD, data quality, disaster recovery, data lineage, data governance</p>
        <p><strong>Machine Learning / AI:</strong> machine learning, artificial intelligence, predictive modeling, audience segmentation, recommendation systems, feature engineering, MLOps, model deployment, agentic AI, LLM integration, Jupyter Notebooks, Snowflake Intelligence, Snowpark</p>
        <p><strong>Data Visualization &amp; Business Intelligence:</strong> Qlik Sense, Zoho Analytics, Sigma Computing, Power BI, Tableau, Excel, dashboard development, data storytelling, KPI reporting, self-service analytics</p>
        <p><strong>Platforms, Languages &amp; Tools:</strong> Python, SQL, pandas, NumPy, Matplotlib, scikit-learn, PyTorch, SQL Server, PostgreSQL, Git, CI/CD pipelines, Fivetran, Snowflake, AWS, Azure, cloud data platforms</p>
        <p><strong>Leadership &amp; Collaboration:</strong> Agile, Scrum, technical leadership, requirements gathering, stakeholder management, cross-functional collaboration, technical mentorship, data governance</p>
        <p><strong>Domain Expertise:</strong> nonprofit fundraising, CRM systems, donor analytics, nonprofit analytics, predictive customer behavior modeling, marketing analytics, financial services, financial modeling, risk analysis</p>
        <h1>Education</h1>
        <h2><strong>University of Texas at Dallas, BS in Applied Mathematics</strong> -- Dallas, TXJune 2014 – Dec 2016</h2>
        <ul>
        <li>Minor in Philosophy</li>
        </ul>
        <h2><strong>Collin College, AS in General Science</strong> -- Plano, TXJan 2013 – May 2014</h2>
        <ul>
        <li>Graduated Summa Cum Laude</li>
        </ul>
        <h1>Experience</h1>
        <h2><strong>Lead Data Engineer, Virtuous Software</strong> -- Phoenix, AZ (Remote)</h2>
        <p>May 2025 – Dec 2025</p>
        <p>8 months</p>
        <p>Led the design, delivery, and operational maturity of Virtuous' core data platform, owning end-to-end pipelines that powered analytics, reporting, and ML/AI initiatives for new data products.</p>
        <ul>
        <li>
        <p><strong>Built three production AI systems</strong> including automated dbt documentation agent handling schema drift reducing manual engineering maintenance of documentation and tests, intelligent bug triage chatbot reducing incident response time 50%, and Snowflake Intelligence NLQ interface democratizing analytics access company-wide and cutting analyst workload 35%</p>
        </li>
        <li>
        <p>Consolidated ingestion architecture from 200+ individual models to unified framework-- <strong>improving scalability 300% while reducing infrastructure costs 45% and establishing 99% reliability</strong> across all downstream systems</p>
        </li>
        <li>
        <p><strong>Raised test coverage from 5% to 95%</strong> across customer-facing assets, achieving near-zero production escapes, establishing new engineering quality baseline, and <strong>raised ERD documentation coverage from 15% to 90%</strong> using automated dbt doc generation via Snowflake metadata to expose both internally and externally to customers as an enablement resource</p>
        </li>
        <li>
        <p>Partnered with Platform team to remediate critical SQL Server security vulnerabilities, eliminating production hard-delete access, that prevented potential production data loss and provided solutions for architecture improvements in SQL Server for future growth</p>
        </li>
        <li>
        <p><strong>Drove Data Products roadmap alignment and technical prioritization</strong> during rapid org and platform change, ML/AI experimentation, and feature development</p>
        </li>
        <li>
        <p><strong>Mentored and managed 4+ engineers</strong> through comprehensive onboarding, 30/60/90 day growth plans, and daily development activities, <strong>creating force multipliers across the organization</strong></p>
        </li>
        </ul>
        <h2><strong>Senior Data Engineer, Virtuous Software</strong> -- Phoenix, AZ (Remote)</h2>
        <p>Aug 2024 – May 2025</p>
        <p>10 months</p>
        <p>Designed, built, and optimized production-grade data pipelines and analytics models that powered product analytics, customer insights, and foundational ML/AI use cases across Virtuous' SaaS platform.</p>
        <ul>
        <li>
        <p><strong>Architected and implemented end-to-end, scalable ELT pipelines</strong> using Snowflake, dbt, and SQL to ingest multiple data provider platforms and translate raw data into standardized schemas for analytics engineering, experimentation, data sharing, and ML/AI feature development</p>
        </li>
        <li>
        <p>Delivered the Contact Engagement Dataset, a core analytics asset powering new customer-facing Insights features and <strong>directly enabling revenue-generating product capabilities</strong></p>
        </li>
        <li>
        <p>Established data quality checks, testing strategies, and observability patterns to <strong>reduce pipeline failures and increase trust in production analytics outputs</strong></p>
        </li>
        <li>
        <p><strong>Integrated third-party data enrichment sources</strong> via modular Snowflake + dbt pipelines (using APIs, Blob Storage for .csv extraction, etc.) improving data completeness and downstream modeling flexibility</p>
        </li>
        <li>
        <p><strong>Collaborated cross-functionally with software engineers, data scientists, product managers, and analysts</strong> to define metrics, data contracts, and source-of-truth models—preventing production run failures and misaligned reporting</p>
        </li>
        <li>
        <p>Optimized data models and transformations for performance and cost efficiency, <strong>reducing query latency, improving warehouse utilization, and lower overall Snowflake costs</strong></p>
        </li>
        </ul>
        <h2><strong>Principal Engineer, Dunham+Company</strong> -- Plano, TX (Hybrid)</h2>
        <p>Apr 2022 – Aug 2024</p>
        <p>2 years 5 months</p>
        <p>Served as technical owner of Dunham Insights, a homegrown SaaS analytics platform for nonprofit fundraising intelligence and targeted donor cultivation, from data ingestion through product delivery.</p>
        <ul>
        <li>
        <p>Architected and led <strong>development of a multi-tenant analytics platform used by nonprofit clients worldwide</strong>, playing a key role in the platform’s growth, analytical direction, and eventual successful acquisition after just two years on the market</p>
        </li>
        <li>
        <p>Designed <strong>robust data ingestion and transformation pipelines across disparate client data sources</strong> (Virtuous, Blackbaud, Salesforce, MailChimp, Google Analytics, etc.)</p>
        </li>
        <li>
        <p>Partnered with leadership and stakeholders to translate fundraising strategy into actionable analytics products, <strong>providing more sophisticated insights than industry-standard reporting tools</strong></p>
        </li>
        <li>
        <p>Set engineering <strong>standards for reliability, scalability, and long-term maintainability</strong> and trained junior engineers on best practices in data engineering and analytics</p>
        </li>
        <li>
        <p>Redesigned agency's standard multi-channel segmentation model to <strong>improve lapsed donor reactivation 25% using ML predictive scoring and refined feature engineering</strong></p>
        </li>
        </ul>
        <h2><strong>Data Integration Specialist, Dunham+Company</strong> -- Plano, TX (Hybrid)</h2>
        <p>Jan 2022 – Apr 2022</p>
        <p>4 months</p>
        <p>Focused on onboarding, normalizing, and validating complex nonprofit data sources to ensure accuracy and consistency across analytics workflows.</p>
        <ul>
        <li>
        <p><strong>Led complete cloud migration from on-premises SQL Server</strong> to AWS/Snowflake/dbt, delivering 40% cost reduction while improving performance, integration capabilities, and reliability</p>
        </li>
        <li>
        <p>Built <strong>repeatable data validation and transformation logic to improve the quality and stakeholder trust</strong> of downstream analytics</p>
        </li>
        <li>
        <p>Worked directly with internal teams to troubleshoot data issues, edge cases, and custom marketing segmentation models</p>
        </li>
        <li>
        <p>Accelerated client onboarding by standardizing ingestion and QA processes, and <strong>expanded data customization within standard dashboard templates</strong> with user defined marketing source mappings for actionable insights</p>
        </li>
        </ul>
        <h2><strong>Data Analyst, Dunham+Company</strong> -- Plano, TX (Hybrid)</h2>
        <p>Mar 2021 – Jan 2022</p>
        <p>11 months</p>
        <p>Delivered analytical insights and reporting that supported nonprofit fundraising strategy and campaign optimization.</p>
        <ul>
        <li>
        <p><strong>Created dashboard suite and analyses to surface donor behavior, trends, and performance</strong> in white-labeled Qlik Sense SaaS experience</p>
        </li>
        <li>
        <p>Created Data Audit service, <strong>generating direct revenue for Data Science department and improving data quality for clients</strong> through systematic data validation processes and best-practice recommendations</p>
        </li>
        <li>
        <p>Built predictive analytics models and multi-channel segmentation engine with user-interface to democratize segmentation data pulls to marketing strategists, <strong>improving targeting accuracy by 35%, driving measurable client ROI, and reducing Data Science department task load by 30%</strong></p>
        </li>
        <li>
        <p>Established re-usable visualization standards in Qlik Sense SaaS, <strong>reducing dashboard development and client onboarding 70%</strong></p>
        </li>
        <li>
        <p>Spearheaded Agile transformation of Data Science Department, <strong>increasing team velocity 57%</strong> through sprint planning and continuous delivery practices</p>
        </li>
        </ul>
        <h2><strong>Junior Data Analyst, Dunham+Company</strong> -- Plano, TX (Hybrid)</h2>
        <p>Sept 2020 – Mar 2021</p>
        <p>7 months</p>
        <p>Began career supporting analytics and reporting efforts while rapidly expanding technical and domain expertise in nonprofit data.</p>
        <ul>
        <li>
        <p><strong>Prepared ad hoc reports in SQL, Excel, and Zoho Analytics</strong> to support client fundraising campaigns and strategic decision-making</p>
        </li>
        <li>
        <p>Developed standard Zoho Analytics reporting suite to automate recurring client reporting tasks, <strong>reducing Data Science department workload 70% thus allowing focus on higher-impact infrastructure projects</strong></p>
        </li>
        <li>
        <p>Developed a reputation for <strong>fast learning, curiosity, and strong attention to detail</strong></p>
        </li>
        <li>
        <p>Laid groundwork for <strong>rapid progression into advanced analytics and engineering roles</strong>, showing initiative to simplify data processes and improve data quality</p>
        </li>
        </ul>
        <h1>Selected Honors</h1>
        <ul>
        <li>
        <p>Summa Cum Laude, Collin College (2014)</p>
        </li>
        <li>
        <p>SOA Exam P/ CAS Exam 1 (2020)</p>
        </li>
        </ul>
        <h1>Publications</h1>
        <h2><strong>Data Health Tips</strong></h2>
        <p>Apr 2022</p>
        <p>Practical guide outlining five foundational practices to improve data health and governance within organizations, focusing on consistency in CRM usage, structuring critical data elements, standardized campaign tagging, minimizing problematic characters in data values, and avoiding default placeholder entries to preserve data quality. These steps are designed to enhance analytics accuracy, streamline reporting, and increase the value of data-driven insights across systems and teams.</p>
        <p><em>Jennifer Baber</em></p>
        <p><a href="https://www.dunhamandcompany.com/data-health-tips/">www.dunhamandcompany.com/data-health-tips</a></p>
        <h2><strong>The Importance of Visualizations</strong></h2>
        <p>Sept 2021</p>
        <p>Explores the strategic role of data visualization in transforming complex datasets into clear, actionable insights. Highlights best practices for effective visual storytelling, including choosing appropriate chart types, emphasizing clarity over decoration, and designing visuals that align with business questions—enabling stakeholders to interpret data quickly, make informed decisions, and drive measurable impact.</p>
        <p><em>Jennifer Baber</em></p>
        <p><a href="https://www.dunhamandcompany.com/the-importance-of-visualizations/">www.dunhamandcompany.com/the-importance-of-visualizations</a></p>
    </article>
</body>

</html>
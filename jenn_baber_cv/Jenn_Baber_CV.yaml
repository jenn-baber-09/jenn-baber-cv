# yaml-language-server: $schema=https://raw.githubusercontent.com/rendercv/rendercv/refs/tags/v2.6/schema.json
cv:
  name: Jennifer Baber
  headline: Lead Data Engineer | Cloud Data Architect | AI-Powered Data Platform Enthusiast
  location: Richardson, TX
  email: jennifer.baber@outlook.com
  photo: 
  phone: '+19727421453'
  website: 
  social_networks:
    - network: LinkedIn
      username: jennifer-baber-6969781b0
    - network: GitHub
      username: jenn-baber-09
  sections:
    
    Overview: 
      - Lead Data Engineer with a strong mathematics foundation and a product-driven mindset, specializing in scalable data pipelines, analytics engineering, and ML/AI-ready data platforms. Experienced in Snowflake, dbt, Python, SQL, ETL/ELT, data modeling, and DataOps, with a track record of building production-grade systems that are reliable, cost-efficient, and trusted by stakeholders. Strong cross-functional partner known for modernizing and maturing data architectures, enabling machine learning use cases, and delivering business-impactful insights at scale. 
    
    Technical Proficiencies:
      - label: Data Engineering
        details: dbt, Snowflake, ETL, ELT, data pipelines, data modeling, dimensional modeling, data contracts, DataOps, CI/CD, data quality, disaster recovery, data lineage, data governance
      - label: Machine Learning / AI
        details: machine learning, artificial intelligence, predictive modeling, audience segmentation, recommendation systems, feature engineering, MLOps, model deployment, agentic AI, LLM integration, Jupyter Notebooks, Snowflake Intelligence, Snowpark
      - label: Data Visualization & Business Intelligence
        details: Qlik Sense, Zoho Analytics, Sigma Computing, Power BI, Tableau, Excel, dashboard development, data storytelling, KPI reporting, self-service analytics
      - label: Platforms, Languages & Tools
        details: Python, SQL, pandas, NumPy, Matplotlib, scikit-learn, PyTorch, SQL Server, PostgreSQL, Git, CI/CD pipelines, Fivetran, Snowflake, AWS, Azure, cloud data platforms
      - label: Leadership & Collaboration
        details: Agile, Scrum, technical leadership, requirements gathering, stakeholder management, cross-functional collaboration, technical mentorship, data governance
      - label: Domain Expertise
        details: nonprofit fundraising, CRM systems, donor analytics, nonprofit analytics, predictive customer behavior modeling, marketing analytics, financial services, financial modeling, risk analysis

    experience:
      - company: Virtuous Software
        position: Lead Data Engineer
        start_date: 2025-05
        end_date: 2025-12
        location: Phoenix, AZ (Remote)
        summary: >
          Led the design, delivery, and operational maturity of Virtuous' core data
          platform, owning end-to-end pipelines that powered analytics, reporting,
          and ML/AI initiatives for new data products.
        highlights:
          - "Built three production AI systems including automated dbt documentation agent handling schema drift reducing manual engineering maintenance of documentation and tests, intelligent bug triage chatbot reducing incident response time 50%, and Snowflake Intelligence NLQ interface democratizing analytics access company-wide and cutting analyst workload 35%"
          - "Consolidated ingestion architecture from 200+ individual models to unified framework-- improving scalability 300% while reducing infrastructure costs 45% and establishing 99% reliability across all downstream systems"
          - "Raised test coverage from 5% to 95% across customer-facing assets, achieving near-zero production escapes, establishing new engineering quality baseline, and raised ERD documentation coverage from 15% to 90% using automated dbt doc generation via Snowflake metadata to expose both internally and externally to customers as an enablement resource"
          - "Partnered with Platform team to remediate critical SQL Server security vulnerabilities, eliminating production hard-delete access, that prevented potential production data loss and provided solutions for architecture improvements in SQL Server for future growth"
          - "Drove Data Products roadmap alignment and technical prioritization during rapid org and platform change, ML/AI experimentation, and feature development"
          - "Mentored and managed 4+ engineers through comprehensive onboarding, 30/60/90 day growth plans, and daily development activities, creating force multipliers across the organization"
          

      - company: Virtuous Software
        position: Senior Data Engineer
        start_date: 2024-08
        end_date: 2025-05
        location: Phoenix, AZ (Remote)
        summary: >
          Designed, built, and optimized production-grade data pipelines and analytics
          models that powered product analytics, customer insights, and foundational
          ML/AI use cases across Virtuous' SaaS platform.
        highlights:
          - "Architected and implemented end-to-end, scalable ELT pipelines using Snowflake, dbt, and SQL to ingest multiple data provider platforms and translate raw data into standardized schemas for analytics engineering, experimentation, data sharing, and ML/AI feature development"
          - "Delivered the Contact Engagement Dataset, a core analytics asset powering new customer-facing Insights features and directly enabling revenue-generating product capabilities"
          - "Established data quality checks, testing strategies, and observability patterns to reduce pipeline failures and increase trust in production analytics outputs"
          - "Integrated third-party data enrichment sources via modular Snowflake + dbt pipelines (using APIs, Blob Storage for .csv extraction, etc.) improving data completeness and downstream modeling flexibility"
          - "Collaborated cross-functionally with software engineers, data scientists, product managers, and analysts to define metrics, data contracts, and source-of-truth models—preventing production run failures and misaligned reporting"
          - "Optimized data models and transformations for performance and cost efficiency, reducing query latency, improving warehouse utilization, and lower overall Snowflake costs"


      - company: Dunham+Company
        position: Principal Engineer
        start_date: 2022-04
        end_date: 2024-08
        location: Plano, TX (Hybrid)
        summary: >
          Served as technical owner of Dunham Insights, a homegrown SaaS analytics
          platform for nonprofit fundraising intelligence and targeted donor cultivation, from data ingestion
          through product delivery.
        highlights:
          - "Architected and led development of a multi-tenant analytics platform used by nonprofit clients worldwide, playing a key role in the platform’s growth, analytical direction, and eventual successful acquisition after just two years on the market"
          - "Designed robust data ingestion and transformation pipelines across disparate client data sources (Virtuous, Blackbaud, Salesforce, MailChimp, Google Analytics, etc.)"
          - "Partnered with leadership and stakeholders to translate fundraising strategy into actionable analytics products, providing more sophisticated insights than industry-standard reporting tools"
          - "Set engineering standards for reliability, scalability, and long-term maintainability and trained junior engineers on best practices in data engineering and analytics"
          - "Redesigned agency's standard multi-channel segmentation model to improve lapsed donor reactivation 25% using ML predictive scoring and refined feature engineering"

      - company: Dunham+Company
        position: Data Integration Specialist
        start_date: 2022-01
        end_date: 2022-04
        location: Plano, TX (Hybrid)
        summary: >
          Focused on onboarding, normalizing, and validating complex nonprofit data
          sources to ensure accuracy and consistency across analytics workflows.
        highlights:
          - "Led complete cloud migration from on-premises SQL Server to AWS/Snowflake/dbt, delivering 40% cost reduction while improving performance, integration capabilities, and reliability"
          - "Built repeatable data validation and transformation logic to improve the quality and stakeholder trust of downstream analytics"
          - "Worked directly with internal teams to troubleshoot data issues, edge cases, and custom marketing segmentation models"
          - "Accelerated client onboarding by standardizing ingestion and QA processes, and expanded data customization within standard dashboard templates with user defined marketing source mappings for actionable insights"

      - company: Dunham+Company
        position: Data Analyst
        start_date: 2021-03
        end_date: 2022-01
        location: Plano, TX (Hybrid)
        summary: >
          Delivered analytical insights and reporting that supported nonprofit
          fundraising strategy and campaign optimization.
        highlights:
          - "Created dashboard suite and analyses to surface donor behavior, trends, and performance in white-labeled Qlik Sense SaaS experience"
          - "Created Data Audit service, generating direct revenue for Data Science department and improving data quality for clients through systematic data validation processes and best-practice recommendations"
          - "Built predictive analytics models and multi-channel segmentation engine with user-interface to democratize segmentation data pulls to marketing strategists, improving targeting accuracy by 35%, driving measurable client ROI, and reducing Data Science department task load by 30%"
          - "Established re-usable visualization standards in Qlik Sense SaaS, reducing dashboard development and client onboarding 70%"
          - "Spearheaded Agile transformation of Data Science Department, increasing team velocity 57% through sprint planning and continuous delivery practices"

      - company: Dunham+Company
        position: Junior Data Analyst
        start_date: 2020-09
        end_date: 2021-03
        location: Plano, TX (Hybrid)
        summary: >
          Began career supporting analytics and reporting efforts while rapidly
          expanding technical and domain expertise in nonprofit data.
        highlights:
          - "Prepared ad hoc reports in SQL, Excel, and Zoho Analytics to support client fundraising campaigns and strategic decision-making"
          - "Developed standard Zoho Analytics reporting suite to automate recurring client reporting tasks, reducing Data Science department workload 70% thus allowing focus on higher-impact infrastructure projects"
          - "Developed a reputation for fast learning, curiosity, and strong attention to detail"
          - "Laid groundwork for rapid progression into advanced analytics and engineering roles, showing initiative to simplify data processes and improve data quality"

    selected_honors:
      - bullet: Summa Cum Laude, Collin College (2014)
      - bullet: SOA Exam P/ CAS Exam 1 (2020)

    education:
      - institution: University of Texas at Dallas
        area: Applied Mathematics
        degree: BS
        date:
        start_date: 2014-06
        end_date: 2016-12
        location: Dallas, TX
        summary:
        highlights:
          - Minor in Philosophy
      - institution: Collin College
        area: General Science
        degree: AS
        date:
        start_date: 2013-01
        end_date: 2014-05
        location: Plano, TX
        summary:
        highlights:
          - Graduated Summa Cum Laude

    publications:
      - title: "Data Health Tips"
        authors: 
          - "*Jennifer Baber*"
        publisher: Dunham+Company Blog
        date: 2022-04
        url: "https://www.dunhamandcompany.com/data-health-tips/"
        summary: >
          Practical guide outlining five foundational practices to improve data health and governance within organizations, focusing on consistency in CRM usage, structuring critical data elements, standardized campaign tagging, minimizing problematic characters in data values, and avoiding default placeholder entries to preserve data quality. These steps are designed to enhance analytics accuracy, streamline reporting, and increase the value of data-driven insights across systems and teams.
      - title: "The Importance of Visualizations"
        authors: 
          - "*Jennifer Baber*"
        publisher: Dunham+Company Blog
        date: 2021-09
        url: "https://www.dunhamandcompany.com/the-importance-of-visualizations/"
        summary: >
          Explores the strategic role of data visualization in transforming complex datasets into clear, actionable insights. Highlights best practices for effective visual storytelling, including choosing appropriate chart types, emphasizing clarity over decoration, and designing visuals that align with business questions—enabling stakeholders to interpret data quickly, make informed decisions, and drive measurable impact.
          


design:

  theme: engineeringresumes

  page:
    show_footer: false
    size: us-letter
    show_top_note: true
  #   top_margin: 0.7in
  #   bottom_margin: 0.7in
  #   left_margin: 0.7in
  #   right_margin: 0.7in
  #   show_footer: true

  colors:
    body: rgb(14, 20, 27)           # deep ink
    name: rgb(180, 83, 9)           # burnished copper
    headline: rgb(31, 41, 51)       # charcoal slate
    connections: rgb(31, 41, 51)    # charcoal slate
    section_titles: rgb(180, 83, 9) # burnished copper
    links: rgb(180, 83, 9)          # burnished copper
    footer: rgb(110, 110, 110)
    top_note: rgb(110, 110, 110)

  typography:
    #  line_spacing: 0.6em
    #  alignment: justified
    #  date_and_location_column_alignment: right
    font_family:
      body: XCharter
      name: XCharter
      headline: XCharter
      connections: XCharter
      section_titles: XCharter
    font_size:
      name: "25pt"
      section_titles: "1.2em"
      body: "10pt"
    bold:
      name: false
      headline: true
      connections: false
      section_titles: false
    small_caps:
      name: false
      headline: false
      connections: false
      section_titles: false

  links:
    underline: false
    show_external_link_icon: false

  header:
    alignment: left
    # photo_width: 3.5cm
    # photo_position: left
    # photo_space_left: 0.4cm
    # photo_space_right: 0.4cm
    # space_below_name: 0.7cm
    # space_below_headline: 0.7cm
    # space_below_connections: 0.7cm
    connections:
      separator: ""
      show_icons: true
      display_urls_instead_of_usernames: false
      phone_number_format: national
      hyperlink: true
      space_between_connections: 0.3cm
      
  section_titles:
    type: with_full_line
    space_above: "0.5cm"
    space_below: "0.3cm"
    line_thickness: 0.5pt

  sections:
    allow_page_break: true
    space_between_regular_entries: "0.42cm"
    space_between_text_based_entries: "0.15cm"
    show_time_spans_in:
      - experience
  
  entries:
    # date_and_location_width: 4.15cm
    # side_space: 0.2cm
    # space_between_columns: 0.1cm
    # allow_page_break: false
    short_second_row: false
    summary:
      space_above: 0.08cm
      #   space_left: 0cm
    side_space: "0cm"
    highlights:
      bullet: ●
      nested_bullet: ●
      space_left: "0cm"
      space_above: 0.08cm
      space_between_items: 0.08cm
      space_between_bullet_and_text: 0.3em

  templates:
    education_entry:
      main_column: "**INSTITUTION, DEGREE in AREA** -- LOCATION\nSUMMARY\nHIGHLIGHTS"
      date_and_location_column: DATE
      degree_column: null
    normal_entry:
      main_column: "**NAME** -- **LOCATION**\nSUMMARY\nHIGHLIGHTS"
      date_and_location_column: DATE
    experience_entry:
      main_column: "**POSITION, COMPANY** -- LOCATION\nSUMMARY\nHIGHLIGHTS"
      date_and_location_column: DATE

locale:
  language: english
  last_updated: Last updated
  # month: month
  # months: months
  # year: year
  # years: years
  # present: present
  # month_abbreviations:
  #   - Jan
  #   - Feb
  #   - Mar
  #   - Apr
  #   - May
  #   - June
  #   - July
  #   - Aug
  #   - Sept
  #   - Oct
  #   - Nov
  #   - Dec
  # month_names:
  #   - January
  #   - February
  #   - March
  #   - April
  #   - May
  #   - June
  #   - July
  #   - August
  #   - September
  #   - October
  #   - November
  #   - December

settings:
  current_date: '2026-01-21'
  render_command:
    design: 
    locale:
    typst_path: .\NAME_IN_SNAKE_CASE_CV.typ
    pdf_path: .\NAME_IN_SNAKE_CASE_CV.pdf
    markdown_path: .\NAME_IN_SNAKE_CASE_CV.md
    html_path: .\NAME_IN_SNAKE_CASE_CV.html
    png_path: .\NAME_IN_SNAKE_CASE_CV.png
    dont_generate_markdown: false
    dont_generate_html: false
    dont_generate_typst: false
    dont_generate_pdf: false
    dont_generate_png: true
  bold_keywords: [
    'Lead Data Engineer with a strong mathematics foundation and a product-driven mindset,', 
    'a track record of building production-grade systems that are reliable, cost-efficient, and trusted by stakeholders',
    'Built three production AI systems',
    'improving scalability 300% while reducing infrastructure costs 45% and establishing 99% reliability',
    'Raised test coverage from 5% to 95%',
    'raised ERD documentation coverage from 15% to 90%',
    'remediate critical SQL Server security vunerabilities, eliminating production hard-delete access, that prevented potential production data loss',
    'Drove Data Products roadmap alignment and technical prioritization',
    'Mentored and managed 4+ engineers',
    'creating force multipliers across the organization',
    'Architected and implemented end-to-end, scalable ELT pipelines',
    'directly enabling revenue-generating product capabilities',
    'reduce pipeline failures and increase trust in production analytics outputs',
    'Integrated third-party data enrichment sources',
    'Collaborated cross-functionally with software engineers, data scientists, product managers, and analysts',
    'reducing query latency, improving warehouse utilization, and lower overall Snowflake costs',
    'development of a multi-tenant analytics platform used by nonprofit clients worldwide',
    'robust data ingestion and transformation pipelines across disparate client data sources',
    'providing more sophisticated insights than industry-standard reporting tools',
    'standards for reliability, scalability, and long-term maintainability',
    'improve lapsed donor reactivation 25% using ML predictive scoring and refined feature engineering',
    'Led complete cloud migration from on-premises SQL Server',
    'repeatable data validation and transformation logic to improve the quality and stakeholder trust',
    'expanded data customization within standard dashboard templates',
    'Created dashboard suite and analyses to surface donor behavior, trends, and performance',
    'generating direct revenue for Data Science department and improving data quality for clients',
    'improving targeting accuracy by 35%, driving measurable client ROI, and reducing Data Science department task load by 30%',
    'reducing dashboard development and client onboarding 70%',
    'increasing team velocity 57%',
    'Prepared ad hoc reports in SQL, Excel, and Zoho Analytics',
    'reducing Data Science department workload 70% thus allowing focus on higher-impact infrastructure projects',
    'fast learning, curiosity, and strong attention to detail',
    'rapid progression into advanced analytics and engineering roles'
  ]